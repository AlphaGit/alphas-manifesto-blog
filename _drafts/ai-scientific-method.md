There's an article published in TheNewStack.io that is titled _[Machine Learning Algorithm Sidesteps the scientific method](https://thenewstack.io/machine-learning-algorithm-sidesteps-the-scientific-method/)_. While I do consider this to be clickbait, I still consider some truth behind it worth discussing.

## How does it sidestep the scientific method?

First of all, you might wonder why they say that a certain ML algorithm _sidesteps_  the scientific method, and if that's actually possible at all. They explain that there's this algorithm that learned to predict planetary motions based only on observational data and field theory.

> “It is worthwhile to emphasize that the serving and learning algorithms do not know, learn, or use Newton’s laws of motion and universal gravitation,” wrote the team. “The discrete field theory directly connects the observational data and new predictions. Newton’s laws are not needed.”

Now, based on this they say that this algorithm is escaping the limits of the scientific method because it does like we did it on the old days: sees data, predicts data. We verify it's true and we got new scientific knowledge. No need to study other theories, no need to incorporate existing knowledge.

That's still a bit of a stretch. After all, it _is_ basing itself in field theory, meaning that we're assuming it to be true. It is also assuming the observed data to be true. All of these things are considered in the scientific method so that experiments can be replicated and verified. After all, being right just once is valuable but not enough to be called "science", is it?

In short, what this team did is replicate an old-school Galileo that would run experiments on his own and make predictions that made sense. Which is still a great feat!

## Why is this news at all?

Being able to predict without relying on existing knowledge allows for predictions to be wrong or right _despite_ them. We will come to a point where these predictions will be consistently right even when our current scientific knowledge says they shouldn't, and so we will be able to learn about the universe from what a computer found out. As such, I agree that this is a _Big Deal™_.

Also, the idea of sidestepping existing knowledge is a very powerful one too. It means that there's a lot of less bootstrap required to have these models running. Assuming that computing powera and memory storage becomes cheaper and more accesible over time, these three things will make scientific discoveries available for anyone that can feed data to computers and have them compute predictions.

This alone could bring us back into the golden age of science, with discoveries being made all of the time and advancing our knowledge of the world.

## Philosophical changes

What's more, having shareable machine models that work the predictions out are a technological version of the formulas that science had been creating for years. There is still a few differences: formulas can be passed down in paper, while machine models require storage (which is almost ubiquitous today). Also, models are a lot more difficult to interpret and make sense out of than a formula might. This hinders some of the theoretical advances that gave such a big push to physics: the ability to play around with mathematical concepts without having to experiment directly.

This ability helped physics (and other fields) proliferate without the need of direct experimentation. However, when experimentation and verification was not possible, we saw the rise of multiple competing interpretations of the world, theories over theories of how the universe worked while not being able to easily discard each of these options.

These machine learning models would not really bring anything new to the mix: you can just as well have a bunch of scientist try to predict patterns and come up with predictions until we find a new piece of knowledge. But this work is arduous and slow, which is why leaving it to machines is a good investment.

As such, I believe that sciences (not only physics) will see a bigger disconnect between their experimental and theoretical counterparts.

This advance might help us pass down knowledge in the form of algorithms that depend only on data. This is, we might be able to see farther away without standing on the shoulders of giants.